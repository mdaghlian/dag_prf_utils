#!/usr/bin/env python
#$ -j Y
#$ -cwd
#$ -V

import sys
import os
opj = os.path.join
import numpy as np
import yaml
import pickle
from datetime import datetime, timedelta
import time

try:
    from prfpy_csenf.stimulus import PRFStimulus2D
    from prfpy_csenf.model import Iso2DGaussianModel
    from prfpy_csenf.fit import Iso2DGaussianFitter
except:
    from prfpy.stimulus import PRFStimulus2D
    from prfpy.model import Iso2DGaussianModel
    from prfpy.fit import Iso2DGaussianFitter
from dag_prf_utils.utils import *
from dag_prf_utils.prfpy_functions import *

def main(argv):
    '''
    ---------------------------
    rapid prf fitting

    give design matrix file, ts file, and yml file 
    
    Args:
        --ts_file       folder with .gii files
        --dm_file       design matrix
        --yml_file      yml file
        --output_dir    where to put the output files

        ...
        TODO: other options
    ---------------------------
    '''
    sub = None
    dm_file = None
    ts_file = None
    yml_file = None
    output_dir = None
    model = 'gauss'
    n_jobs = 10
    overwrite = False
    grid_only = False
    ow_prf_settings = {} # overwrite prf settings from the yml file with these settings

    for i,arg in enumerate(argv):
        if '--sub' in arg:
            sub = argv[i+1]
        elif '--ts_file' in arg:
            ts_file = argv[i+1]        
        elif '--dm_file' in arg:
            dm_file = argv[i+1]                
        elif '--yml_file' in arg:
            yml_file = argv[i+1]
        elif '--output_dir' in arg:
            output_dir = argv[i+1]
        elif arg in ('--overwrite', '--ow'):
            overwrite=True
        elif arg in ("--tc", "--bgfs", "--nelder"):
            constraints = arg.split('--')[-1]
        elif arg in ("--grid_only", ):
            grid_only = True
        elif arg in ('-h', '--help'):
            print(main.__doc__)
            sys.exit(2)
        elif '--' in arg:
            ow_prf_settings[arg.split('--')[-1]] = dag_arg_checker(argv[i+1])    
    
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
    out = f"{sub}_{dag_hyphen_parse('model', model)}_roi-all-fits"    
    # Load the yaml settings
    with open(yml_file, 'r') as f:
        prf_settings = yaml.safe_load(f)
    # Add important info to settings
    prf_settings['sub'] = sub
    prf_settings['model'] = model
    prf_settings['n_jobs'] = n_jobs
    prf_settings['constraints'] = constraints
    prf_settings['prf_out'] = out 
    prf_settings['prf_dir'] = output_dir
    if len(ow_prf_settings)>0:
        for key in ow_prf_settings.keys():
            prf_settings[key] = ow_prf_settings[key]
            print(f'Overwriting {key} with {ow_prf_settings[key]}')
    for k in prf_settings.keys():
        if prf_settings[k] == 'None':
            prf_settings[k] = None
    fit_hrf = prf_settings.get('fit_hrf', False)
    cut_vols = prf_settings.get('cut_vols', 0)
    
    # Load the time series
    ts_data = np.load(ts_file)

    if ts_data.shape[0]<ts_data.shape[1]:
        ts_data = ts_data.T

    # Load the design matrix 
    try:
        dm = np.load(dm_file)
    except:
        import scipy.io
        dm = scipy.io.loadmat(dm_file)
        # Find the key with an array
        for key in dm.keys():
            if isinstance(dm[key], np.ndarray):
                dm = dm[key]
                break
    
    print(ts_data.shape)
    print(dm.shape)
    assert dm.shape[-1]==ts_data.shape[-1]
    prf_stim = PRFStimulus2D(
        screen_size_cm=prf_settings['screen_size_cm'],          # Distance of screen to eye
        screen_distance_cm=prf_settings['screen_distance_cm'],  # height of the screen (i.e., the diameter of the stimulated region)
        design_matrix=dm,                            # dm (npix x npix x time_points)
        TR=prf_settings['TR'],                                  # TR
        )   
    max_eccentricity = prf_stim.screen_size_degrees/2 # It doesn't make sense to look for PRFs which are outside the stimulated region 
    gg = Iso2DGaussianModel(
        stimulus=prf_stim,                                  # The stimulus we made earlier
        hrf=prf_settings['hrf']['pars'],                    # These are the parameters for the HRF that we normally use at Spinoza (with 7T data). (we can fit it, this will be done later...)
        normalize_RFs=prf_settings['normalize_RFs'],        # Normalize the volume of the RF (so that RFs w/ different sizes have the same volume. Generally not needed, as this can be solved using the beta values i.e.,amplitude)
        )
    gf = Iso2DGaussianFitter(
        data=ts_data,             # time series
        model=gg,                       # model (see above)
        n_jobs=prf_settings['n_jobs'], # number of jobs to use in parallelization 
        )    

    # <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<< IF NOT DONE - DO GRID FIT
    # CHECK FOR TOTAL grid fit...
    # Check has it been run for *all grids*    
    grid_gauss = dag_find_file_in_folder([sub, model, 'gauss', 'grid'], output_dir, return_msg=None, )            

    if (grid_gauss is None) or (overwrite):
        print('Not done grid fit - doing that now')
        g_start_time = datetime.now().strftime('%Y-%m-%d_%H-%M')
        print(f'Starting grid {g_start_time}')
        start = time.time()
        #        
        grid_nr = prf_settings['grid_nr'] # Size of the grid (i.e., number of possible PRF models). Higher number means that the grid fit will be more exact, but take longer...
        eccs    = max_eccentricity * np.linspace(0.25, 1, grid_nr)**2 # Squared because of cortical magnification, more efficiently tiles the visual field...
        sizes   = max_eccentricity * np.linspace(0.1, 1, grid_nr)**2  # Possible size values (i.e., sigma in gaussian model) 
        polars  = np.linspace(0, 2*np.pi, grid_nr*4)              # Possible polar angle coordinates

        # We can also fit the hrf in the same way (specifically the derivative)
        # -> make a grid between 0-10 (see settings file)
        if fit_hrf:
            hrf_1_grid = np.linspace(prf_settings['hrf']['deriv_bound'][0], prf_settings['hrf']['deriv_bound'][1], int(grid_nr/2))
            # We generally recommend to fix the dispersion value to 0
            hrf_2_grid = np.array([0.0])        
        else:
            hrf_1_grid = None
            hrf_2_grid = None

        # *** NOTE we will overwrite the HRF parameters for AS1, AS2 tasks -> & use those fit in AS0 *** 
        gauss_grid_bounds = [prf_settings['prf_ampl']] 
        print(prf_settings['fixed_grid_baseline'])
        print(type(prf_settings['fixed_grid_baseline']))

        gf.grid_fit(
            ecc_grid=eccs,
            polar_grid=polars,
            size_grid=sizes,
            hrf_1_grid=hrf_1_grid,
            hrf_2_grid=hrf_2_grid,
            verbose=True,
            n_batches=prf_settings['n_jobs'],                          # The grid fit is performed in parallel over n_batches of units.Batch parallelization is faster than single-unit parallelization and of sequential computing.
            fixed_grid_baseline=prf_settings['fixed_grid_baseline'],    # Fix the baseline? This makes sense if we have fixed the baseline in preprocessing
            grid_bounds=gauss_grid_bounds,
            )
        # Proccess the fit parameters... (make the shape back to normals )
        gf.gridsearch_params = dag_filter_for_nans(gf.gridsearch_params)            
        g_end_time = datetime.now().strftime('%Y-%m-%d_%H-%M')
        elapsed = (time.time() - start)
        
        # Stuff to print:         
        print(f'Finished grid {g_end_time}')
        print(f'Took {timedelta(seconds=elapsed)}')
        vx_gt_rsq_th = gf.gridsearch_params[:,-1]>prf_settings['rsq_threshold']
        nr_vx_gt_rsq_th = np.mean(vx_gt_rsq_th) * 100
        mean_vx_gt_rsq_th = np.mean(gf.gridsearch_params[vx_gt_rsq_th,-1])
        print(f'Percent of vx above rsq threshold: {nr_vx_gt_rsq_th}. Mean rsq for threshold vx {mean_vx_gt_rsq_th}')

        # Save everything as a pickle...
        grid_pkl_file = opj(output_dir, f'{out}_stage-grid_desc-prf_params.pkl')
        # Put them in the correct format to save
        grid_pars_to_save = gf.gridsearch_params
        grid_dict = {}
        grid_dict['pars'] = grid_pars_to_save
        grid_dict['settings'] = prf_settings
        grid_dict['start_time'] = g_start_time
        grid_dict['end_time'] = g_end_time
        grid_dict['prfpy_model'] = gg

        f = open(grid_pkl_file, "wb")
        pickle.dump(grid_dict, f)
        f.close()
    else:
        print('Loading old grid parameters')
        with open(grid_gauss, 'rb') as f:
            grid_dict = pickle.load(f)
        # Apply the mask 
        gf.gridsearch_params = grid_dict['pars']        

    vx_gt_rsq_th = gf.gridsearch_params[:,-1]>prf_settings['rsq_threshold']
    nr_vx_gt_rsq_th = np.mean(vx_gt_rsq_th) * 100
    mean_vx_gt_rsq_th = np.mean(gf.gridsearch_params[vx_gt_rsq_th,-1])
    print(f'Percent of vx above rsq threshold: {nr_vx_gt_rsq_th}. Mean rsq for threshold vx {mean_vx_gt_rsq_th}')

    print(f'Mean rsq = {gf.gridsearch_params[:,-1].mean():.3f}')
    if grid_only:
        print('ONLY GRID!!!')
        return        
    # ************************************************************************
    
    # <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<< DO ITERATIVE FIT
    print([out, 'gauss', 'iter', constraints])
    iter_check = dag_find_file_in_folder([out, 'gauss', 'iter', dag_hyphen_parse('constr', constraints)], output_dir, return_msg=None)
    if (iter_check is not None) and (not overwrite):
        print(f'Already done {iter_check}')
        sys.exit()        

    gauss_bounds = [
        (-1.5*max_eccentricity, 1.5*max_eccentricity),          # x bound
        (-1.5*max_eccentricity, 1.5*max_eccentricity),          # y bound
        (1e-1, max_eccentricity*3),                             # prf size bounds
        (prf_settings['prf_ampl'][0],prf_settings['prf_ampl'][1]),      # prf amplitude
        (prf_settings['bold_bsl'][0],prf_settings['bold_bsl'][1]),      # bold baseline (fixed)
        (prf_settings['hrf']['deriv_bound'][0], prf_settings['hrf']['deriv_bound'][1]), # hrf_1 bound
        (prf_settings['hrf']['disp_bound'][0],  prf_settings['hrf']['disp_bound'][1]), # hrf_2 bound
    ]
    print(gauss_bounds)

    # Constraints determines which scipy fitter is used
    # -> can also be used to make certain parameters interdependent (e.g. size depening on eccentricity... not normally done)
    if prf_settings['constraints']=='tc':
        g_constraints = []   # uses trust-constraint (slower, but moves further from grid
        minimize_args = {}
    elif prf_settings['constraints']=='bgfs':
        g_constraints = None # uses l-BFGS (which is faster)
        minimize_args = {}
    elif prf_settings['constraints']=='nelder':
        g_constraints = []
        minimize_args = dict(
            method='nelder-mead',            
            options=dict(disp=False),
            constraints=[],
            tol=float(prf_settings['ftol']),
            )
        
    i_start_time = datetime.now().strftime('%Y-%m-%d_%H-%M')
    print(f'Starting iter {i_start_time}, constraints = {g_constraints}')
    start = time.time()

    gf.iterative_fit(
        rsq_threshold=prf_settings['rsq_threshold'],    # Minimum variance explained. Puts a lower bound on the quality of PRF fits. Any fits worse than this are thrown away...     
        verbose=True,
        bounds=gauss_bounds,       # Bounds (on parameters)
        constraints=g_constraints, # Constraints
        xtol=float(prf_settings['xtol']),     # float, passed to fitting routine numerical tolerance on x
        ftol=float(prf_settings['ftol']),     # float, passed to fitting routine numerical tolerance on function
        minimize_args=minimize_args,
        )

    # Fiter for nans
    gf.iterative_search_params = dag_filter_for_nans(gf.iterative_search_params)    
    i_end_time = datetime.now().strftime('%Y-%m-%d_%H-%M')
    print(f'End iter {i_end_time}')           
    elapsed = (time.time() - start)
    print(f'Finished iter {i_end_time}')
    print(f'Took {timedelta(seconds=elapsed)}')
    vx_gt_rsq_th = gf.iterative_search_params[:,-1]>prf_settings['rsq_threshold']
    nr_vx_gt_rsq_th = np.mean(vx_gt_rsq_th) * 100
    mean_vx_gt_rsq_th = np.mean(gf.iterative_search_params[vx_gt_rsq_th,-1]) 
    print(f'Percent of vx above rsq threshold: {nr_vx_gt_rsq_th}. Mean rsq for threshold vx {mean_vx_gt_rsq_th}')

    # *************************************************************


    # Save everything as a pickle...
    iter_pars_to_save = gf.iterative_search_params   
    iter_pkl_file = opj(output_dir, f'{out}_stage-iter_constr-{constraints}_desc-prf_params.pkl')
    iter_dict = {}
    iter_dict['pars'] = iter_pars_to_save
    iter_dict['settings'] = prf_settings
    iter_dict['start_time'] = i_start_time
    iter_dict['end_time'] = i_end_time
    iter_dict['prfpy_model'] = gg

    from figure_finder.utils import get_running_path, get_running_code_string
    iter_dict['running_code_string'] = get_running_code_string(get_running_path())

    # Dump everything!!! into pickle
    f = open(iter_pkl_file, "wb")
    pickle.dump(iter_dict, f)
    f.close()
    # Dump running code 
    iter_pkl_file = opj(output_dir, f'{out}_stage-iter_constr-{constraints}_desc-prf_params.pkl')
    run_code_file = iter_pkl_file.replace('prf_params.pkl', 'running_code.py')
    with open(run_code_file, 'w') as f:
        f.write(iter_dict['running_code_string'])
    
    # Also dump the settings as a separate yaml file for ease of reading 
    settings_file = iter_pkl_file.replace('prf_params.pkl', 'settings.yml')
    with open(settings_file, 'w') as f:
        yaml.dump(prf_settings, f)


    print('DONE!!!')



if __name__ == "__main__":
    main(sys.argv[1:])    